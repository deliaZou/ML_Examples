## 线性回归

##### 一次函数定义公式

$$
y(x,w)=w_0+w_1*x
$$

##### 平方损失函数公式

$$
\sum_{i=1}^{n}(y_i−(w_0+w_1*x_i))^2
$$

##### 最小二乘法代数解（推导略）

$$
w_1=\frac{(n∑x_iy_i−(∑x_i∑y_i))}{n∑x_i^2−(∑xi)^2}
$$

$$
w_0=\frac{∑x_i^2∑y_i−∑x_i∑x_iy_i}{n∑x_i^2−(∑xi)^2}
$$

##### 最小二乘法矩阵解（推导略）

$$
W=(X^TX)^{-1})X^Ty
$$

### 多项式回归

##### 一元高阶多项式定义

$$
y(x,w)=w_0+w_1x+w_2x^2+…+w_mx^m=\sum_{j=0}^{m}w_jx^j
$$

##### 损失函数

$$
\sum_{i=1}^{n}y_i−y'_i
$$

### 岭回归

##### 线性回归的损失函数   (在简单版本上加上了 L2 正则项（2-范数） )

$$
F_{Ridge}=\sum_{i=1}^{n}(y_i−w^Tx)^2+λ\sum_{i=1}^{n}(w_i)^2
$$

##### 损失函数最优解（#λ:正则化强度，上同）

$$
w_{Ridge}=(X^TX+λI)^{−1}X^TY
$$

### Lasso回归

##### 损失函数 （(在简单版本上加上了 L1 正则项（1-范数） )

$$
F_{LASSO}=\sum_{i=1}^{n}(y_i−w^Tx)^2+λ\sum_{i=1}^{n}|w_i|
$$

### 模型评估检验

##### MAE（绝对误差平均值）

$$
\operatorname{MAE}(y, \hat{y})=\frac{1}{n} \sum_{i=1}^{n}\left|y_{i}-\hat{y}_{i}\right|
$$

##### MSE（ 误差的平方的期望值 ）

$$
\textrm{MSE}(y, \hat{y} ) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y})^{2}
$$

##### MAPE （ 预测结果较真实结果平均偏离 ）

$$
\textrm{MAPE}(y, \hat{y} ) = \frac{\sum_{i=1}^{n}{|\frac{y_{i}-\hat y_{i}}{y_{i}}|}}{n} \times 100
$$



##### 拟合优度检验

实际值vs实际值平均值vs预测线上的值
$$
TSS=\sum_{i=1}^{n} Y_{i}^{2}=\sum_{i=1}^{n}\left(y_{i}-\overline{y}\right)^{2}
$$

$$
E S S=\sum_{i=1}^{n} \hat{Y}_{i}^{2}=\sum_{i=1}^{n}\left(\hat{y}_{i}-\overline{y}\right)^{2}
$$

$$
RSS=\sum_{i=1}^{n} e_{i}^{2}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}
$$

以上三式，可得
$$
TSS=ESS+RSS
$$
 TSS 总体平方和 Total Sum of Squares，ESS 回归平方和 Explained Sum of Squares， RSS 残差平方和 Residual Sum of Squares 

###### 拟合优度：

$$
R^{2}=\frac{ESS}{TSS}=1-\frac{RSS}{TSS}
$$

#当 RSS 越小时，R2R^{2}R2 就越趋近于 1，那么代表模型的解释力越强。反之，模型的解释力就越弱。所以，一般情况下，R2R^{2}R2 的有效取值范围在 [0,1][0, 1][0,1] 之间。值越大，就代表模型的拟合优度越好。 

## 逻辑回归

由2个函数构成1，回归函数2，逻辑函数（Sigmoid）
$$
z_{i} = {w_0}{x_0} + {w_1}{x_1} + \cdots + {w_i}{x_i} = {w^T}x
$$

$$
f(z_{i})=\frac{1}{1+e^{-z_{i}}}
$$



### 感知机

##### Sign函数

$$
\operatorname{sign}(x)=\left\{\begin{array}{ll}{+1,} & {\text { if } x \geq 0} \\ {-1,} & {\text { if }  x<0}\end{array}\right.
$$

##### 损失函数

$$
J(W,b) = - \sum_{x_i\epsilon M} y_i(W*x_{i}+b)
$$

### K-近邻算法（KNN）

##### 最近邻算法（NN）

最近邻算法（Nearest Neighbor，简称：NN）：其针对未知类别数据 x，在训练集中找到与 x 最相似的训练样本 *y*，用 y的样本对应的类别作为未知类别数据 x 的类别，从而达到分类的效果。--->拓展  K-近邻算法

K-近邻 也是机器学习分类算法中最简单的方法之一。KNN 算法的核心思想和最近邻算法思想相似，都是通过寻找和未知样本相似的类别进行分类。但 NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好。

##### 算法实现

1. 数据准备：通过数据清洗，数据处理，将每条数据整理成向量。  

2. 计算距离：计算测试数据与训练数据之间的距离。*  

3. 寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。  

4. 决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。

   *计算距离

   - 曼哈顿距离（马氏距离）2个点横纵坐标差的和，计算简单
   - 欧式距离，通常意义上的直线距离 ，三角公式计算

   *决策规则

   - 多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。  
   - 加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。

##### sklearn算法

```
sklearn.neighbors.KNeighborsClassifier((n_neighbors=5, weights='uniform', algorithm='auto')
```

- `n_neighbors` : `k` 值，表示邻近个数，默认为 `5`。
- `weights` : 决策规则选择，多数表决或加权表决，可用参数（`'uniform'`,`'distance'`）
- `algorithm` : 搜索算法选择（`auto`，`kd_tree`, `ball_tree`），包括逐一搜索，`kd` 树搜索或 `ball` 树搜索

##### KD树

当数据量特别大时，KNN的计算会非常耗时。为了提高 KNN 搜索效率，减少计算距离的次数，可以通过构建 Kd 树的方法提高计算效率。

Kd 树（英文：K-dimension tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。Kd  树是一种二叉树，表示对 K 维空间的一个划分，构造 Kd 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K  维超矩形区域。Kd 树的每个结点对应于一个 K 维超矩形区域。利用 Kd 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。

##### Kd 树最邻近搜索

以下便是 Kd 树的最邻近搜索步骤：

- 从根节点开始，递归的往下移。往左还是往右的决定方法与插入元素的方法一样(如果输入点在分区面的左边则进入左子节点，在右边则进入右子节点)。  
- 一旦移动到叶节点，将该节点当作「目前最佳点」。 
- 解开递归，并对每个经过的节点运行下列步骤：  
  - 如果目前所在点比目前最佳点更靠近输入点，则将其变为目前最佳点。  
  - 检查另一边子树有没有更近的点，如果有则从该节点往下找  
- 当根节点搜索完毕后完成最邻近搜索

通过步骤可以十分直观的发现，相比于传统 KNN 搜索遍历要节约很多时间和空间。

##### KD树算法实现

SKlearn已经集成：

sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')  # kd 树搜索

### 朴素贝叶斯（Naive Bayes）

##### 贝叶斯定理

$$
P(B \mid A)=\frac{P(AB)}{P(A)}=\frac{P(A \mid B) \times P(B)}{P(A)}
$$

先验概率：P(A),P(B)； 后验概率：P(A|B),P(B|A)

##### 朴素贝叶斯

$$
P(\text{类别} \mid \text{特征})=\frac{P(\text{特征} \mid \text{类别}) \times P(\text{类别})}{P(\text{特征})} 
$$

朴素贝叶斯中的「朴素」，即条件独立，表示其假设预测的各个属性都是相互独立的,每个属性独立地对分类结果产生影响，条件独立在数学上的表示为：*P*(*A**B*)=*P*(*A*)×*P*(*B*)P(AB)=P(A)×P(B) 。这样，使得朴素贝叶斯算法变得简单，但有时会牺牲一定的分类准确率。对于预测数据，求解在该预测数据的属性出现时各个类别的出现概率，将概率值大的类别作为预测数据的类别。

##### 贝叶斯估计

$$
P(y_{i}=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda }{N+k\lambda}
$$

其中 λ≥0 等价于在随机变量各个取值的频数上赋予一个正数，当 λ=0 时就是极大似然估计。在平时常取λ=1，这时称为拉普拉斯平滑。

例如：生成数据中，
$$
P(Y=A)=\frac{8+1}{15+2*1}=\frac{9}{17}
$$
取 λ=1 此时由于一共有 AB 两个类别，则 k 取 2。

同样计算 P(特征∣种类) 时，也是给计算时的分子分母加上拉普拉斯平滑。例如：生成数据中，
$$
P(x_{1}=r \mid Y=A)=\frac{4+1}{8+3*1}=\frac{5}{11}
$$
同样取λ=1 此时由于 x 中有 r, g, b 三个种类，所以这里 k 取值为 3。

##### 朴素贝叶斯的三种常见模型

- 多项式模型

  当特征值为离散时，常常使用多项式模型。事实上，在以上实验的参数估计中，我们所应用的就是多项式模型。为避免概率值为 0 的情况出现，多项式模型采用的是贝叶斯估计。

- 伯努利模型

  与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是 1 和 0（以文本分类为例，某个单词在文档中出现过，则其特征值为 1，否则为 0）。                                                                       

  在伯努利模型中，条件概率 P(x(i)∣y(k))的计算方式为：

  - 当特征值 x(i)=1时，P((x(i) |y(k))=P(x(i)=1 | y(k)) ;  
  - 当特征值 x(i)=0时，P((x(i) |y(k))=P(x(i)=0 | y(k)) 。

- 高斯模型

  当特征是连续变量的时候，在不做平滑的情况下，运用多项式模型就会导致很多 P(x(i)∣y(k))=0，此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，采用高斯模型。高斯模型是假设连续变量的特征数据是服从高斯分布的，高斯分布函数表达式为：
  $$
  P(x_{i}|y_{k})=\frac{1}{\sqrt{2\pi}\sigma_{y_{k},i}}exp(-\frac{(x-\mu_{y_{k},i}) ^{2}}{2\sigma ^{2}_{y_{k}},i})
  $$

  - μ 表示类别为 y(k) 的样本中，第 i 维特征的均值。  
  - σ 表示类别为 y(k) 的样本中，第 i 维特征的方差。  

