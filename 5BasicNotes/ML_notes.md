## 线性回归

##### 一次函数定义公式

$$
y(x,w)=w_0+w_1*x
$$

##### 平方损失函数公式

$$
\sum_{i=1}^{n}(y_i−(w_0+w_1*x_i))^2
$$

##### 最小二乘法代数解（推导略）

$$
w_1=\frac{(n∑x_iy_i−(∑x_i∑y_i))}{n∑x_i^2−(∑xi)^2}
$$

$$
w_0=\frac{∑x_i^2∑y_i−∑x_i∑x_iy_i}{n∑x_i^2−(∑xi)^2}
$$

##### 最小二乘法矩阵解（推导略）

$$
W=(X^TX)^{-1})X^Ty
$$

### 多项式回归

##### 一元高阶多项式定义

$$
y(x,w)=w_0+w_1x+w_2x^2+…+w_mx^m=\sum_{j=0}^{m}w_jx^j
$$

##### 损失函数

$$
\sum_{i=1}^{n}y_i−y'_i
$$

### 岭回归

##### 线性回归的损失函数   (在简单版本上加上了 L2 正则项（2-范数） )

$$
F_{Ridge}=\sum_{i=1}^{n}(y_i−w^Tx)^2+λ\sum_{i=1}^{n}(w_i)^2
$$

##### 损失函数最优解（#λ:正则化强度，上同）

$$
w_{Ridge}=(X^TX+λI)^{−1}X^TY
$$

### Lasso回归

##### 损失函数 （(在简单版本上加上了 L1 正则项（1-范数） )

$$
F_{LASSO}=\sum_{i=1}^{n}(y_i−w^Tx)^2+λ\sum_{i=1}^{n}|w_i|
$$

### 模型评估检验

##### MAE（绝对误差平均值）

$$
\operatorname{MAE}(y, \hat{y})=\frac{1}{n} \sum_{i=1}^{n}\left|y_{i}-\hat{y}_{i}\right|
$$

##### MSE（ 误差的平方的期望值 ）

$$
\textrm{MSE}(y, \hat{y} ) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y})^{2}
$$

##### MAPE （ 预测结果较真实结果平均偏离 ）

$$
\textrm{MAPE}(y, \hat{y} ) = \frac{\sum_{i=1}^{n}{|\frac{y_{i}-\hat y_{i}}{y_{i}}|}}{n} \times 100
$$



##### 拟合优度检验

实际值vs实际值平均值vs预测线上的值
$$
TSS=\sum_{i=1}^{n} Y_{i}^{2}=\sum_{i=1}^{n}\left(y_{i}-\overline{y}\right)^{2}
$$

$$
E S S=\sum_{i=1}^{n} \hat{Y}_{i}^{2}=\sum_{i=1}^{n}\left(\hat{y}_{i}-\overline{y}\right)^{2}
$$

$$
RSS=\sum_{i=1}^{n} e_{i}^{2}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}
$$

以上三式，可得
$$
TSS=ESS+RSS
$$
 TSS 总体平方和 Total Sum of Squares，ESS 回归平方和 Explained Sum of Squares， RSS 残差平方和 Residual Sum of Squares 

###### 拟合优度：

$$
R^{2}=\frac{ESS}{TSS}=1-\frac{RSS}{TSS}
$$

#当 RSS 越小时，R2R^{2}R2 就越趋近于 1，那么代表模型的解释力越强。反之，模型的解释力就越弱。所以，一般情况下，R2R^{2}R2 的有效取值范围在 [0,1][0, 1][0,1] 之间。值越大，就代表模型的拟合优度越好。 

## 逻辑回归

由2个函数构成1，回归函数2，逻辑函数（Sigmoid）
$$
z_{i} = {w_0}{x_0} + {w_1}{x_1} + \cdots + {w_i}{x_i} = {w^T}x
$$

$$
f(z_{i})=\frac{1}{1+e^{-z_{i}}}
$$



### 感知机

##### Sign函数

$$
\operatorname{sign}(x)=\left\{\begin{array}{ll}{+1,} & {\text { if } x \geq 0} \\ {-1,} & {\text { if }  x<0}\end{array}\right.
$$

##### 损失函数

$$
J(W,b) = - \sum_{x_i\epsilon M} y_i(W*x_{i}+b)
$$

### K-近邻算法（KNN）

##### 最近邻算法（NN）

最近邻算法（Nearest Neighbor，简称：NN）：其针对未知类别数据 x，在训练集中找到与 x 最相似的训练样本 *y*，用 y的样本对应的类别作为未知类别数据 x 的类别，从而达到分类的效果。--->拓展  K-近邻算法

K-近邻 也是机器学习分类算法中最简单的方法之一。KNN 算法的核心思想和最近邻算法思想相似，都是通过寻找和未知样本相似的类别进行分类。但 NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好。

##### 算法实现

1. 数据准备：通过数据清洗，数据处理，将每条数据整理成向量。  

2. 计算距离：计算测试数据与训练数据之间的距离。*  

3. 寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。  

4. 决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。

   *计算距离

   - 曼哈顿距离（马氏距离）2个点横纵坐标差的和，计算简单
   - 欧式距离，通常意义上的直线距离 ，三角公式计算

   *决策规则

   - 多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。  
   - 加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。

##### sklearn算法

```
sklearn.neighbors.KNeighborsClassifier((n_neighbors=5, weights='uniform', algorithm='auto')
```

- `n_neighbors` : `k` 值，表示邻近个数，默认为 `5`。
- `weights` : 决策规则选择，多数表决或加权表决，可用参数（`'uniform'`,`'distance'`）
- `algorithm` : 搜索算法选择（`auto`，`kd_tree`, `ball_tree`），包括逐一搜索，`kd` 树搜索或 `ball` 树搜索

##### KD树

当数据量特别大时，KNN的计算会非常耗时。为了提高 KNN 搜索效率，减少计算距离的次数，可以通过构建 Kd 树的方法提高计算效率。

Kd 树（英文：K-dimension tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。Kd  树是一种二叉树，表示对 K 维空间的一个划分，构造 Kd 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K  维超矩形区域。Kd 树的每个结点对应于一个 K 维超矩形区域。利用 Kd 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。

##### Kd 树最邻近搜索

以下便是 Kd 树的最邻近搜索步骤：

- 从根节点开始，递归的往下移。往左还是往右的决定方法与插入元素的方法一样(如果输入点在分区面的左边则进入左子节点，在右边则进入右子节点)。  
- 一旦移动到叶节点，将该节点当作「目前最佳点」。 
- 解开递归，并对每个经过的节点运行下列步骤：  
  - 如果目前所在点比目前最佳点更靠近输入点，则将其变为目前最佳点。  
  - 检查另一边子树有没有更近的点，如果有则从该节点往下找  
- 当根节点搜索完毕后完成最邻近搜索

通过步骤可以十分直观的发现，相比于传统 KNN 搜索遍历要节约很多时间和空间。

##### KD树算法实现

SKlearn已经集成：

sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')  # kd 树搜索